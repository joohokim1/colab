{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNqUG8WClYlX5721EGjxM1+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 환경 설정\n","- 공유 드라이브에서 ipynb 파일 다운로드\n","- colab.research.google.com 접속\n","- 파일 > 노트 업로드 > 다운 받은 ipynb 파일 업로드\n","- 수정 > 노트 설정 > 하드웨어 가속기 GPU 여부 확인\n","\n","## Huggingface\n","- 각종 딥러닝 모델 구현을 위한 transformers 라이브러리\n","  - https://huggingface.co/docs/transformers/index\n","- 공개 모델 및 데이터셋 플랫폼 제공\n","  - https://huggingface.co/models\n","  - https://huggingface.co/datasets\n","\n","## 실습 1-1. PyTorch를 이용한 Fine-tuning\n","- PyTorch 기본 기능을 이용해서 리뷰 분류 모델을 학습\n","- yelp 데이터셋 (\"yelp_review_full\") : 식당 리뷰 데이터. 리뷰 문장과 긍정, 부정 레이블로 구성"],"metadata":{"id":"gyLroVwnI6rP"}},{"cell_type":"code","source":["# 필요한 라이브러리 설치\n","!pip install transformers datasets evaluate -q"],"metadata":{"id":"Z__9ufQ2r0Xu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import evaluate\n","\n","from tqdm.auto import tqdm\n","from torch.optim import AdamW\n","from datasets import load_dataset\n","from torch.utils.data import DataLoader\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler"],"metadata":{"id":"vtWy4oZVuV2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# yelp review 데이터셋 사용\n","# train set 1,000개. test set 100개 샘플링\n","dataset = load_dataset(\"yelp_review_full\")\n","small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n","small_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(100))"],"metadata":{"id":"yrtrNbVtuZy_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["small_train_dataset[100]"],"metadata":{"id":"mwS61chCurkF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터셋 로컬에 저장하기\n","small_eval_dataset.to_json(\"data.json\")"],"metadata":{"id":"6ONtq-XTY5mW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!head data.json"],"metadata":{"id":"e_xwny_sZcFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 로컬 파일을 데이터셋으로 불러오기\n","sample_dataset_for_loading = load_dataset(\"json\", data_files={\"test\": \"data.json\"})\n","sample_dataset_for_loading"],"metadata":{"id":"hpas0vFdZgCd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# bert tokenizer 설정\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n","\n","# tokenization\n","tokenized_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n","tokenized_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)"],"metadata":{"id":"FEbwMznKt3Zr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(tokenized_train_dataset), len(tokenized_train_dataset)"],"metadata":{"id":"FRx6-kycv_qm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(tokenized_train_dataset[0])"],"metadata":{"id":"yc_kmpMNwK0i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_train_dataset[0].keys()"],"metadata":{"id":"7BUf8LDGwTO9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k, v in tokenized_train_dataset[0].items():\n","    print(k, v)"],"metadata":{"id":"wyzxMrC6wV_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tokenized dataset을 torch에서 사용 가능한 포맷으로 처리\n","train_tensor = tokenized_train_dataset.remove_columns([\"text\"])\n","train_tensor = train_tensor.rename_column(\"label\", \"labels\")\n","train_tensor.set_format(\"torch\")\n","\n","eval_tensor = tokenized_eval_dataset.remove_columns([\"text\"])\n","eval_tensor = eval_tensor.rename_column(\"label\", \"labels\")\n","eval_tensor.set_format(\"torch\")"],"metadata":{"id":"-t6vSRvRwefC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tensor[0].keys()"],"metadata":{"id":"OUmR2n7HxOT4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k, v in train_tensor[0].items():\n","    if k == \"labels\":\n","        print(k, v)\n","    else:\n","        print(k, v[:200])"],"metadata":{"id":"U4QTkSXixvLo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataloader, model, optimizer 설정\n","train_dataloader = DataLoader(train_tensor, shuffle=True, batch_size=8)\n","eval_dataloader = DataLoader(eval_tensor, batch_size=8)\n","model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n","optimizer = AdamW(model.parameters(), lr=5e-5)"],"metadata":{"id":"onWX3xxLyTf_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scheduler 설정\n","num_epochs = 1\n","num_training_steps = num_epochs * len(train_dataloader)\n","lr_scheduler = get_scheduler(\n","    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",")"],"metadata":{"id":"mKgzyhZey-y7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model device 설정\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","model.to(device)"],"metadata":{"id":"pn10JQGOzE29"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 성능 평가\n","def eval_model():\n","    metric = evaluate.load(\"accuracy\")\n","    model.eval()\n","\n","    for batch in eval_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","\n","        logits = outputs.logits\n","        predictions = torch.argmax(logits, dim=-1)\n","        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n","\n","    return metric.compute()"],"metadata":{"id":"pLI-In197RZ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 전 성능 확인\n","eval_model()"],"metadata":{"id":"qPAx1OJW54Sw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 학습\n","progress_bar = tqdm(range(num_training_steps))\n","model.train()\n","\n","for epoch in range(num_epochs):\n","    for batch in train_dataloader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        loss.backward()\n","\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","        progress_bar.update(1)"],"metadata":{"id":"Int0ykvfzRix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습 후 성능 확인\n","eval_model()"],"metadata":{"id":"MVUBBl2C7eRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 output 확인용 샘플\n","raw = tokenized_eval_dataset[0]\n","data = eval_tensor[0]\n","\n","for k, v in raw.items():\n","    print(k, v)\n","\n","print()\n","\n","for k, v in data.items():\n","    if k == \"labels\":\n","        print(k, v)\n","    else:\n","        print(k, v[:200])"],"metadata":{"id":"G9x56HpB2RZq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs = model(\n","    input_ids=data[\"input_ids\"].view(1, -1).to(device),\n","    token_type_ids=data[\"token_type_ids\"].view(1, -1).to(device),\n","    attention_mask=data[\"attention_mask\"].view(1, -1).to(device)\n",")\n","outputs.logits.softmax(dim=1)"],"metadata":{"id":"a5v25Dpj4cK4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 실습 1-2. 네이버 영화 리뷰 Fine-tuning\n","- 이전 내용을 참고해서 영화 리뷰 분류 모델을 학습시켜 봅시다.\n","  - 데이터셋 및 모델 설정 -> 학습 전 성능 평가 -> 학습 -> 학습 후 성능 평가\n","- 네이버 영화 리뷰 데이터셋 사용 (\"nsmc\")\n","  - 길이 20자 이상 데이터만 사용\n","  - Train Set 3,000개 샘플링 (seed=42)\n","  - Test Set 100개 샘플링 (seed=42)\n","  - 길이 필터링 -> 셔플링 -> 샘플링\n","- 1 Epoch 학습\n","- 20 step 단위로 누적 평균 loss를 구해봅시다. (loss sum / step count)\n","- 한글 데이터를 처리하기 위해 SKT KoBERT를 사용합니다. (\"skt/kobert-base-v1\")\n","  - https://github.com/SKTBrain/KoBERT"],"metadata":{"id":"4nwocKnTLlwX"}},{"cell_type":"code","source":["# 필요한 라이브러리 설치\n","!pip install transformers datasets evaluate sentencepiece -q\n","!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf' -q"],"metadata":{"id":"-nNXh-hhM4tq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# KoBERT Tokenizer 라이브러리 사용\n","from kobert_tokenizer import KoBERTTokenizer\n","\n","tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')"],"metadata":{"id":"zvaX2-ZONIGn"},"execution_count":null,"outputs":[]}]}